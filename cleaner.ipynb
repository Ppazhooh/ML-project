{"nbformat":4,"nbformat_minor":0,"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.3"},"colab":{"name":"cleaner.ipynb","provenance":[],"collapsed_sections":[]}},"cells":[{"cell_type":"code","metadata":{"id":"iGg_xJ3z3mez"},"source":["# Import general purpose packages\n","import numpy as np \n","import pandas as pd \n","import sys\n","import json\n","import os\n","import re\n","\n","# This is what we are using for data preparation and ML part (thanks, Rafal, for great tutorial)\n","from sklearn.preprocessing import LabelBinarizer\n","from nltk.stem import WordNetLemmatizer"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"C9kdw9Zf3me0"},"source":["artists = pd.read_csv('artists-data.csv')\n","songs = pd.read_csv('lyrics-data.csv') #load the list of songs"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"m7RVbmWt3me1"},"source":["data = pd.DataFrame() \n","G=artists.Genre.unique()\n","for genre in G:\n","    Genre_artists = artists[artists['Genre']==genre] # filter artists by genre\n","    Genre_songs = pd.merge(songs, Genre_artists, how='inner', left_on='ALink', right_on='Link') #inner join of pop artists with songs to get only songs by pop artists\n","    Genre_songs = Genre_songs[['Genre', 'Artist', 'SName', 'Lyric','Idiom']].rename(columns={'SName':'Song'})#leave only columns of interest and rename some of them.\n","    Genre_songs = Genre_songs.dropna() # Remove incomplete records, cleanse lyrics\n","    #Genre_songs = Genre_songs[songs['Lyric']!='Instrumental'].head(SONGS_PER_GENRE).applymap(cleanse) #Remove instrumental compositions  and limit the size of final dataset\n","    data=pd.concat([data, Genre_songs])\n","    \n","    \n","data=data.loc[data['Idiom'] == 'ENGLISH']\n","\n","data.index = range(len(data))\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"cPmbyaEE3me1"},"source":["#Keep only 3 most frequent genrres\n","data=data.loc[data['Genre'].isin(['Rock','Pop','Hip Hop'])]\n","data.index = range(len(data))\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"CUb1UBaY3me1"},"source":["#Convert Categorical genre to binary label\n","GenreBinarizer = LabelBinarizer().fit(data['Genre'])\n","Genre_Label = GenreBinarizer.transform(data['Genre'])\n","\n","\n","Genre_Label_df = pd.DataFrame(Genre_Label, columns =['G_H', 'G_P', 'G_R'])\n","\n","final_data=pd.concat([data, Genre_Label_df], axis=1)\n","\n","##### Shuffle data\n","final_data=final_data.sample(frac=1) \n","final_data.index = range(len(final_data))\n","final_data=final_data.drop(columns=['Genre', 'Artist','Idiom'])"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"S3NNViqk3me1"},"source":["#Creat train,validation and test data\n","train_data=final_data[:87000]\n","validation_data=final_data[87000:111600]\n","test_data=final_data[111600:]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"J_4YwUNw3me1"},"source":["documents = []\n","stemmer = WordNetLemmatizer()\n","for sen in range(0, len(train_data)):\n","    temp=str(train_data['Lyric'].values[sen])\n","    # Remove all the special characters\n","    document = re.sub(r'\\W', ' ', str(temp))\n","    \n","    # remove all single characters\n","    document = re.sub(r'\\s+[a-zA-Z]\\s+', ' ', document)\n","    \n","\n","    \n","    # Substituting multiple spaces with single space\n","    document = re.sub(r'\\s+', ' ', document, flags=re.I)\n","    \n","    # Removing prefixed 'b'\n","    document = re.sub(r'^b\\s+', '', document)\n","    \n","    # Converting to Lowercase\n","    document = document.lower()\n","    \n","    # Lemmatization\n","    document = document.split()\n","\n","    document = [stemmer.lemmatize(word) for word in document]\n","    document = ' '.join(document)\n","    \n","    documents.append(document)\n","    \n","    \n","train_Lyric=documents"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"JADH3ccN3me1"},"source":["documents = []\n","stemmer = WordNetLemmatizer()\n","for sen in range(0, len(validation_data)):\n","    temp=str(validation_data['Lyric'].values[sen])\n","    # Remove all the special characters\n","    document = re.sub(r'\\W', ' ', str(temp))\n","    \n","    # remove all single characters\n","    document = re.sub(r'\\s+[a-zA-Z]\\s+', ' ', document)\n","    \n","\n","    \n","    # Substituting multiple spaces with single space\n","    document = re.sub(r'\\s+', ' ', document, flags=re.I)\n","    \n","    # Removing prefixed 'b'\n","    document = re.sub(r'^b\\s+', '', document)\n","    \n","    # Converting to Lowercase\n","    document = document.lower()\n","    \n","    # Lemmatization\n","    document = document.split()\n","\n","    document = [stemmer.lemmatize(word) for word in document]\n","    document = ' '.join(document)\n","    \n","    documents.append(document)\n","    \n","    \n","validation_Lyric=documents"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"dlndJ9oU3me1"},"source":["documents = []\n","stemmer = WordNetLemmatizer()\n","for sen in range(0, len(test_data)):\n","    temp=str(test_data['Lyric'].values[sen])\n","    # Remove all the special characters\n","    document = re.sub(r'\\W', ' ', str(temp))\n","    \n","    # remove all single characters\n","    document = re.sub(r'\\s+[a-zA-Z]\\s+', ' ', document)\n","    \n","\n","    \n","    # Substituting multiple spaces with single space\n","    document = re.sub(r'\\s+', ' ', document, flags=re.I)\n","    \n","    # Removing prefixed 'b'\n","    document = re.sub(r'^b\\s+', '', document)\n","    \n","    # Converting to Lowercase\n","    document = document.lower()\n","    \n","    # Lemmatization\n","    document = document.split()\n","\n","    document = [stemmer.lemmatize(word) for word in document]\n","    document = ' '.join(document)\n","    \n","    documents.append(document)\n","    \n","    \n","test_Lyric=documents"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"dCuoZ-Vc3me1"},"source":["y_train=train_data[['G_H','G_P','G_R']].to_numpy()\n","y_validation=validation_data[['G_H','G_P','G_R']].to_numpy()\n","y_test=test_data[['G_H','G_P','G_R']].to_numpy()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Jb8vb_ok3me2"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"k_nDOsaW3uQh"},"source":["a random change to push from colab\n","another change!"]}]}