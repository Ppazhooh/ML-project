# -*- coding: utf-8 -*-
"""Bert3

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1pYvdmoJZs6NRP8kklo-O9mkz-ktlIqGl

https://curiousily.com/posts/sentiment-analysis-with-bert-and-hugging-face-using-pytorch-and-python/

## Setup

We'll need [the Transformers library](https://huggingface.co/transformers/) by Hugging Face:
"""

!pip install -q -U watermark

!pip install -qq transformers

# Commented out IPython magic to ensure Python compatibility.
# %reload_ext watermark
# %watermark -v -p numpy,pandas,torch,transformers

# Commented out IPython magic to ensure Python compatibility.
#@title Setup & Config
import transformers
from transformers import BertModel, BertTokenizer, AdamW, get_linear_schedule_with_warmup
import torch

import numpy as np
import pandas as pd
import seaborn as sns
from pylab import rcParams
import matplotlib.pyplot as plt
from matplotlib import rc
from sklearn.model_selection import train_test_split
from sklearn.metrics import confusion_matrix, classification_report
from collections import defaultdict
from textwrap import wrap

from torch import nn, optim
from torch.utils.data import Dataset, DataLoader
import torch.nn.functional as F

# %matplotlib inline
# %config InlineBackend.figure_format='retina'

sns.set(style='whitegrid', palette='muted', font_scale=1.2)

HAPPY_COLORS_PALETTE = ["#01BEFE", "#FFDD00", "#FF7D00", "#FF006D", "#ADFF02", "#8F00FF"]

sns.set_palette(sns.color_palette(HAPPY_COLORS_PALETTE))

rcParams['figure.figsize'] = 12, 8

RANDOM_SEED = 42
np.random.seed(RANDOM_SEED)
torch.manual_seed(RANDOM_SEED)

device = torch.device("cuda:0" if torch.cuda.is_available() else "cpu")
device

"""## Data Exploration
"""

from google.colab import files
files.upload()

!mkdir -p ~/.kaggle
!cp kaggle.json ~/.kaggle/
!chmod 600 ~/.kaggle/kaggle.json

!kaggle datasets download -d neisse/scrapped-lyrics-from-6-genres

!unzip scrapped-lyrics-from-6-genres.zip

!ls

# Import general purpose packages
import numpy as np 
import pandas as pd 
import sys
import json
import os
import re

# This is what we are using for data preparation and ML part (thanks, Rafal, for great tutorial)
from sklearn.preprocessing import LabelBinarizer
from nltk.stem import WordNetLemmatizer

songs = pd.read_csv('lyrics-data.csv') #load the list of songs
artists = pd.read_csv('artists-data.csv')

data = pd.DataFrame() 
G=artists.Genre.unique()
for genre in G:
    Genre_artists = artists[artists['Genre']==genre] # filter artists by genre
    Genre_songs = pd.merge(songs, Genre_artists, how='inner', left_on='ALink', right_on='Link') #inner join of pop artists with songs to get only songs by pop artists
    Genre_songs = Genre_songs[['Genre', 'Artist', 'SName', 'Lyric','Idiom']].rename(columns={'SName':'Song'})#leave only columns of interest and rename some of them.
    Genre_songs = Genre_songs.dropna() # Remove incomplete records, cleanse lyrics
    #Genre_songs = Genre_songs[songs['Lyric']!='Instrumental'].head(SONGS_PER_GENRE).applymap(cleanse) #Remove instrumental compositions  and limit the size of final dataset
    data=pd.concat([data, Genre_songs])
    
    
data=data.loc[data['Idiom'] == 'ENGLISH']

data.index = range(len(data))

#Keep only 3 most frequent genrres
data=data.loc[data['Genre'].isin(['Rock','Pop','Hip Hop'])]
data.index = range(len(data))

#Convert Categorical genre to binary label
GenreBinarizer = LabelBinarizer().fit(data['Genre'])
Genre_Label = GenreBinarizer.transform(data['Genre'])


Genre_Label_df = pd.DataFrame(Genre_Label, columns =['G_H', 'G_P', 'G_R'])

final_data=pd.concat([data, Genre_Label_df], axis=1)

##### Shuffle data
final_data=final_data.sample(frac=1) 
final_data.index = range(len(final_data))
final_data=final_data.drop(columns=['Genre', 'Artist','Idiom'])
final_data['Genre'] = final_data.G_P + final_data.G_R * 2

final_data[:10]

df = final_data.sample(n = 90_000)

len(df)

"""## Data Preprocessing

"""

PRE_TRAINED_MODEL_NAME = 'bert-base-cased'

tokenizer = BertTokenizer.from_pretrained(PRE_TRAINED_MODEL_NAME)

MAX_LEN = 128


class MLyricsDataset(Dataset):

  def __init__(self, lyrics, targets, tokenizer, max_len):
    self.lyrics = lyrics
    self.targets = targets
    self.tokenizer = tokenizer
    self.max_len = max_len
  
  def __len__(self):
    return len(self.lyrics)
  
  def __getitem__(self, item):
    lyr = str(self.lyrics[item])
    target = self.targets[item]

    encoding = self.tokenizer.encode_plus(
      lyr,
      add_special_tokens=True,
      max_length=self.max_len,
      return_token_type_ids=False,
      pad_to_max_length=True,
      return_attention_mask=True,
      return_tensors='pt',
      truncation=True,
      padding='max_length'
    )

    return {
      'lyric_text': lyr,
      'input_ids': encoding['input_ids'].flatten(),
      'attention_mask': encoding['attention_mask'].flatten(),
      'targets': torch.tensor(target, dtype=torch.long)
    }


N_train = 72_000
N_val = 81_000
df_train = df[:N_train]
df_val = df[N_train:N_val]
df_test = df[N_val:]

df_train.shape, df_val.shape, df_test.shape

"""We also need to create a couple of data loaders. Here's a helper function to do it:"""

def create_data_loader(df, tokenizer, max_len, batch_size):
  ds = MLyricsDataset(
    reviews=df.Lyric.to_numpy(),
    targets=df.Genre.to_numpy(),
    tokenizer=tokenizer,
    max_len=max_len
  )

  return DataLoader(
    ds,
    batch_size=batch_size,
    num_workers=4
  )

BATCH_SIZE = 16

train_data_loader = create_data_loader(df_train, tokenizer, MAX_LEN, BATCH_SIZE)
val_data_loader = create_data_loader(df_val, tokenizer, MAX_LEN, BATCH_SIZE)
test_data_loader = create_data_loader(df_test, tokenizer, MAX_LEN, BATCH_SIZE)

"""Let's have a look at an example batch from our training data loader:"""

data = next(iter(train_data_loader))
data.keys()

print(data['input_ids'].shape)
print(data['attention_mask'].shape)
print(data['targets'].shape)

"""##  Classification 
"""

bert_model = BertModel.from_pretrained(PRE_TRAINED_MODEL_NAME)

class GenreClassifier(nn.Module):

  def __init__(self, n_classes):
    super(GenreClassifier, self).__init__()
    self.bert = BertModel.from_pretrained(PRE_TRAINED_MODEL_NAME)
    self.drop = nn.Dropout(p=0.3)
    self.out = nn.Linear(self.bert.config.hidden_size, n_classes)
  
  def forward(self, input_ids, attention_mask):
    _, pooled_output = self.bert(
      input_ids=input_ids,
      attention_mask=attention_mask,
      return_dict=False
    )
    output = self.drop(pooled_output)
    return self.out(output)



model = GenreClassifier(3)
model = model.to(device)

"""### Training

"""

EPOCHS = 4

optimizer = AdamW(model.parameters(), lr=2e-5, correct_bias=False)
total_steps = len(train_data_loader) * EPOCHS

scheduler = get_linear_schedule_with_warmup(
  optimizer,
  num_warmup_steps=0,
  num_training_steps=total_steps
)

loss_fn = nn.CrossEntropyLoss().to(device)

def train_epoch(
  model, 
  data_loader, 
  loss_fn, 
  optimizer, 
  device, 
  scheduler, 
  n_examples
):
  model = model.train()

  losses = []
  correct_predictions = 0
  
  for d in data_loader:
    input_ids = d["input_ids"].to(device)
    attention_mask = d["attention_mask"].to(device)
    targets = d["targets"].to(device)

    outputs = model(
      input_ids=input_ids,
      attention_mask=attention_mask
    )

    _, preds = torch.max(outputs, dim=1)
    loss = loss_fn(outputs, targets)

    correct_predictions += torch.sum(preds == targets)
    losses.append(loss.item())

    loss.backward()
    nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)
    optimizer.step()
    scheduler.step()
    optimizer.zero_grad()

  return correct_predictions.double() / n_examples, np.mean(losses)

def eval_model(model, data_loader, loss_fn, device, n_examples):
  model = model.eval()

  losses = []
  correct_predictions = 0

  with torch.no_grad():
    for d in data_loader:
      input_ids = d["input_ids"].to(device)
      attention_mask = d["attention_mask"].to(device)
      targets = d["targets"].to(device)

      outputs = model(
        input_ids=input_ids,
        attention_mask=attention_mask
      )
      _, preds = torch.max(outputs, dim=1)

      loss = loss_fn(outputs, targets)

      correct_predictions += torch.sum(preds == targets)
      losses.append(loss.item())

  return correct_predictions.double() / n_examples, np.mean(losses)



history = defaultdict(list)
best_accuracy = 0

for epoch in range(EPOCHS):

  print(f'Epoch {epoch + 1}/{EPOCHS}')
  print('-' * 10)

  train_acc, train_loss = train_epoch(
    model,
    train_data_loader,    
    loss_fn, 
    optimizer, 
    device, 
    scheduler, 
    len(df_train)
  )

  print(f'Train loss {train_loss} accuracy {train_acc}')

  val_acc, val_loss = eval_model(
    model,
    val_data_loader,
    loss_fn, 
    device, 
    len(df_val)
  )

  print(f'Val   loss {val_loss} accuracy {val_acc}')
  print()

  history['train_acc'].append(train_acc)
  history['train_loss'].append(train_loss)
  history['val_acc'].append(val_acc)
  history['val_loss'].append(val_loss)

  if val_acc > best_accuracy:
    torch.save(model.state_dict(), 'best_model_state.bin')
    best_accuracy = val_acc



"""## Evaluation

"""

test_acc, _ = eval_model(
  model,
  test_data_loader,
  loss_fn,
  device,
  len(df_test)
)

test_acc.item()



def get_predictions(model, data_loader):
  model = model.eval()
  
  lyric_texts = []
  predictions = []
  prediction_probs = []
  real_values = []

  with torch.no_grad():
    for d in data_loader:

      texts = d["lyric_text"]
      input_ids = d["input_ids"].to(device)
      attention_mask = d["attention_mask"].to(device)
      targets = d["targets"].to(device)

      outputs = model(
        input_ids=input_ids,
        attention_mask=attention_mask
      )
      _, preds = torch.max(outputs, dim=1)

      probs = F.softmax(outputs, dim=1)

      lyric_texts.extend(texts)
      predictions.extend(preds)
      prediction_probs.extend(probs)
      real_values.extend(targets)

  predictions = torch.stack(predictions).cpu()
  prediction_probs = torch.stack(prediction_probs).cpu()
  real_values = torch.stack(real_values).cpu()
  return lyric_texts, predictions, prediction_probs, real_values



y_lyric_texts, y_pred, y_pred_probs, y_test = get_predictions(
  model,
  test_data_loader
)

class_names = ['HipHop', 'Pop', 'Rock']

from sklearn.metrics import f1_score
f1_score(y_test, y_pred, average='micro')


print(classification_report(y_test, y_pred, target_names=class_names))
