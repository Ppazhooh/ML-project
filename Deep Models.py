# -*- coding: utf-8 -*-
"""GloVe_final.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1vbPktHEXLZAJOAGDBAklHF6ViwrxqYMj

#Download Data
"""

from google.colab import files
files.upload()

!mkdir -p ~/.kaggle
!cp kaggle.json ~/.kaggle/
!chmod 600 ~/.kaggle/kaggle.json

!kaggle datasets download -d neisse/scrapped-lyrics-from-6-genres

!unzip scrapped-lyrics-from-6-genres.zip

!rm scrapped-lyrics-from-6-genres.zip

!ls

"""# Exctract Data"""

# Import general purpose packages
import numpy as np 
import pandas as pd 
import sys
import json
import os
import re

# This is what we are using for data preparation and ML part (thanks, Rafal, for great tutorial)
from sklearn.preprocessing import LabelBinarizer
from nltk.stem import WordNetLemmatizer

artists = pd.read_csv('artists-data.csv')
songs = pd.read_csv('lyrics-data.csv') #load the list of songs

data = pd.DataFrame() 
G=artists.Genre.unique()
for genre in G:
    Genre_artists = artists[artists['Genre']==genre] # filter artists by genre
    Genre_songs = pd.merge(songs, Genre_artists, how='inner', left_on='ALink', right_on='Link') #inner join of pop artists with songs to get only songs by pop artists
    Genre_songs = Genre_songs[['Genre', 'Artist', 'SName', 'Lyric','Idiom']].rename(columns={'SName':'Song'})#leave only columns of interest and rename some of them.
    Genre_songs = Genre_songs.dropna() # Remove incomplete records, cleanse lyrics
    #Genre_songs = Genre_songs[songs['Lyric']!='Instrumental'].head(SONGS_PER_GENRE).applymap(cleanse) #Remove instrumental compositions  and limit the size of final dataset
    data=pd.concat([data, Genre_songs])
    
    
data=data.loc[data['Idiom'] == 'ENGLISH']

data.index = range(len(data))

#Keep only 3 most frequent genrres
data=data.loc[data['Genre'].isin(['Rock','Pop','Hip Hop'])]
data.index = range(len(data))

#Convert Categorical genre to binary label
GenreBinarizer = LabelBinarizer().fit(data['Genre'])
Genre_Label = GenreBinarizer.transform(data['Genre'])


Genre_Label_df = pd.DataFrame(Genre_Label, columns =['G_H', 'G_P', 'G_R'])

final_data=pd.concat([data, Genre_Label_df], axis=1)

##### Shuffle data
final_data=final_data.sample(frac=1) 
final_data.index = range(len(final_data))
final_data=final_data.drop(columns=['Genre', 'Artist','Idiom'])

final_data.head()

"""# Clean Data

## Examples
"""

final_data['Lyric'].values[0]

final_data['Lyric'].values[1]

final_data['Lyric'].values[2]

"""## Preprocess"""

def preprocess_text(lyr):
    # Remove punctuations and numbers
    lyr = re.sub('[^a-zA-Z]', ' ', lyr)

    # Single character removal
    lyr = re.sub(r"\s+[a-zA-Z]\s+", ' ', lyr)

    # Removing multiple spaces
    lyr = re.sub(r'\s+', ' ', lyr)


    return lyr

X = []
lyrics = list(final_data['Lyric'])
for lyr in lyrics:
    X.append(preprocess_text(lyr))

y = final_data[['G_H','G_P','G_R']].to_numpy()

len(X)

N_train = 99189
N_val = 111588

X_train, y_train = X[:N_train], y[:N_train]
X_val, y_val = X[N_train:N_val], y[N_train:N_val]
X_test, y_test = X[N_val:], y[N_val:]

"""## Preprocessed Example"""

X[0]

X[1]

"""# Download GloVe"""

!kaggle datasets download -d danielwillgeorge/glove6b100dtxt

!unzip glove6b100dtxt.zip

!ls

from numpy import array
from numpy import asarray
from numpy import zeros

embeddings_dictionary = dict()
glove_file = open('glove.6B.100d.txt', encoding="utf8")

for line in glove_file:
    records = line.split()
    word = records[0]
    vector_dimensions = asarray(records[1:], dtype='float32')
    embeddings_dictionary [word] = vector_dimensions
glove_file.close()



"""# Prepare Embedding Layer"""

import pandas as pd
import numpy as np
import re
import nltk
from nltk.corpus import stopwords
import matplotlib.pyplot as plt


from numpy import array
from keras.preprocessing.text import one_hot
from keras.preprocessing.sequence import pad_sequences
from keras.models import Sequential
from keras.layers.core import Activation, Dropout, Dense
from keras.layers import Flatten
from keras.layers import Conv1D, LSTM
from keras.layers import GlobalMaxPooling1D
from keras.layers.embeddings import Embedding
from sklearn.model_selection import train_test_split
from keras.preprocessing.text import Tokenizer
from keras.utils.np_utils import to_categorical

freqs = pd.Series(' '.join(X).split()).value_counts()

tokenizer = Tokenizer(num_words=20000)
tokenizer.fit_on_texts(X_train)

X_train = tokenizer.texts_to_sequences(X_train)
X_val = tokenizer.texts_to_sequences(X_val)
X_test = tokenizer.texts_to_sequences(X_test)

lengths = [len(x) for x in X_train]
plt.hist(lengths, range=[0, 1000])

sum(1 for l in lengths if l < 600) / len(lengths)

# Adding 1 because of reserved 0 index
vocab_size = len(tokenizer.word_index) + 1

maxlen = 600

X_train = pad_sequences(X_train, padding='post', maxlen=maxlen)
X_val = pad_sequences(X_val, padding='post', maxlen=maxlen)
X_test = pad_sequences(X_test, padding='post', maxlen=maxlen)

embedding_matrix = zeros((vocab_size, 100))
missing = []
for word, index in tokenizer.word_index.items():
    embedding_vector = embeddings_dictionary.get(word)
    if embedding_vector is not None:
        embedding_matrix[index] = embedding_vector
    else:
        missing.append(word)

def get_freq(word):
  if word not in freqs.keys():
    return 0
  return freqs[word]
missing.sort(key=get_freq, reverse=True)
missing[0:20]



"""# Simple NN"""

model = Sequential()
embedding_layer = Embedding(vocab_size, 100, weights=[embedding_matrix], input_length=maxlen , trainable=False)
model.add(embedding_layer)

model.add(Flatten())
model.add(Dense(20, activation='relu'))
model.add(Dense(20, activation='relu'))
model.add(Dense(3, activation='softmax'))

model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['acc'])

print(model.summary())

history = model.fit(X_train, y_train, batch_size=128, epochs=6, verbose=1, validation_data=(X_val, y_val))



"""# CNN"""

model = Sequential()

embedding_layer = Embedding(vocab_size, 100, weights=[embedding_matrix], input_length=maxlen , trainable=False)
model.add(embedding_layer)

model.add(Conv1D(128, 5, activation='relu'))
model.add(GlobalMaxPooling1D())

model.add(Dropout(0.2))
model.add(Dense(10, activation='relu'))
model.add(Dropout(0.2))

model.add(Dense(3, activation='softmax'))
model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['acc'])

print(model.summary())

history = model.fit(X_train, y_train, batch_size=128, epochs=6, verbose=1, validation_data=(X_val, y_val))

history = model.fit(X_train, y_train, batch_size=128, epochs=6, verbose=1, validation_data=(X_val, y_val))

predictions = model.predict(X_test)

from sklearn.metrics import accuracy_score

accuracy_score(y_test.argmax(axis=1), predictions.argmax(axis=1))



from sklearn.metrics import f1_score

f1_score(y_test.argmax(axis=1), predictions.argmax(axis=1), average='micro')

"""# LSTM"""

model = Sequential()
embedding_layer = Embedding(vocab_size, 100, weights=[embedding_matrix], input_length=maxlen , trainable=False)
model.add(embedding_layer)
model.add(LSTM(128))

model.add(Dense(3, activation='softmax'))
model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['acc'])

print(model.summary())

history = model.fit(X_train, y_train, batch_size=128, epochs=18, verbose=1, validation_data=(X_val, y_val))

predictions = model.predict(X_test)

from sklearn.metrics import f1_score

f1_score(y_test.argmax(axis=1), predictions.argmax(axis=1), average='micro')